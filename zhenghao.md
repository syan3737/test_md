> 本期嘉宾：林政豪<br/>
> 话题：深度学习

在过去的几年中，深度神经网络给很多机器学习任务带来了很大的突破。从图像识别到机器翻译，在深度学习的帮助下，很多任务已经实现了超过人类平均水平的精度。然而精度的提升背后的代价是数以百万计的参数，和长达几天甚至几周的显卡上的运算需求。大量的功耗和庞大的模型，限制了深度学习模型在手表手机等小型电子设备上的应用。那么我们该如何在不损失太多精度的同时，给深度网络做减法，让深度学习可以更好的应用在嵌入式系统中呢？本期节目我们从硬件的角度讨论深度学习的优化。




# 提到的一些内容

* [林政豪个人主页](http://cseweb.ucsd.edu/~jel252/)
* 我们提到的第一种减少参数的方法，对weights做剪枝：
[Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/pdf/1506.02626.pdf)
* 节目中多次提到的通过改变连接方式减少参数，增加深度的残差网络ResNet：
[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)
* 林正豪介绍的二元化神经网络的主要内容是基于：[Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1](https://arxiv.org/pdf/1602.02830.pdf)
* 普遍应用于各种神经网络，尤其是对二元化神经网络有决定性作用的batch normalization算法：
[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)


# 怎么收听

* 蜻蜓FM：点击左下角**阅读原文**
* 网易云音乐：[点击这里]()
* 官方网站：[点击这里]()
